\section{Formal Methods and Validation Approach}
\subsection{Methodological Goals}
The methods section serves four goals: (1) make ontology construction
reproducible, (2) justify modeling decisions as answers to explicit research
questions, (3) define objective validation criteria, and (4) connect the
resulting ontology to empirical analysis in hardware-security settings under the
burden-allocation instrument framing.

\subsection{Operational Interpretation}
In this workflow, competency questions (CQs) serve as acceptance criteria for
the model, OWL 2 DL provides formal schema semantics, and reasoner execution
provides consistency and inference checks; SHACL constraints provide explicit
closed-world validation for tuple-level contradiction checks.

\subsection{Ontology Engineering Process}
We follow a competency-question-driven ontology workflow aligned with established
ontology-engineering practice \cite{noy2001ontology,fernandez1997methontology}.
The process proceeds through six stages: specification (scope, stakeholders,
intended users, and non-goals), knowledge acquisition (terms, relations, and
measurement concepts from hardware-security and security-economics literature),
conceptualization (core classes and relation patterns), formalization (OWL 2
encoding of properties and constraints), implementation (versioned
machine-readable artifacts), and iterative evaluation (query testing and model
refinement).

\subsection{Competency Questions}
Competency questions (CQs) define what the ontology must answer
\cite{gruninger1995methodology}. For this paper, CQs are designed to reduce
decision opacity in hardware-security economics by improving cost visibility.
Operationally, they test whether the mechanism-as-instrument framing is
actually queryable.
CQ1 asks for cost visibility by mechanism, stakeholder, lifecycle phase, unit,
evidence grade, and source provenance. CQ2 asks which costs are internalized, transferred, or
externalized, and to whom. CQ3 asks how mechanisms differ under an explicit
objective function (weight regime) and an explicit baseline family. CQ4 asks
which observed incident losses are plausibly linked to missing or weakly
adopted controls, with explicit attribution-evidence type, linkage mechanism,
and counterfactual effect annotation. CQ5 asks which uncertain assumptions most
affect comparative conclusions. CQ6 asks which missing measurements would most
reduce decision uncertainty. CQ7 asks whether burden semantics are consistent
under entailment-style checks (for example, whether \textit{internalized}
actually means bearer equals decision-maker). CQ8 asks whether every
\textit{OpportunityCost} tuple links to an explicit \textit{AlternativeUse}
node and names the foregone resource/benefit and binding design constraint.

\subsection{CQ-to-Model Mapping}
To make evaluation reproducible, each CQ is mapped to explicit conceptual
elements in the ontology. CQ1 maps to mechanism--cost--stakeholder--time
relations with magnitude, unit, evidence, and provenance properties (source key
and source locator). CQ2 maps to
bearing-mode attributes and allocation relations
(internalized/transferred/externalized). CQ3 maps to explicit objective-weight
records and baseline-family selection, producing ranked family deltas under
declared assumptions. CQ4 maps to incident-event entities linked to control
absence/weakness, observed losses, attribution confidence, attribution evidence
type, linkage mechanism, and counterfactual effect. CQ5 maps to uncertainty
annotations and repeatable perturbation analysis over E2/E3 inputs. CQ6 maps to
executable value-of-information (VOI) ranking over family-cost cells using
uncertainty, transfer, and incident-link signals. CQ7 maps to semantic
consistency rules over decision-maker, bearer, and transfer target fields.
CQ8 maps to explicit foregone-alternative descriptors
(alternative-use identifier, foregone resource, foregone benefit, and design
constraint).

\subsection{Formalization and Semantics}
The ontology will be represented in OWL 2 DL to support automated consistency
checking while preserving sufficient expressiveness for class restrictions and
property assertions. The model separates structural concepts (mechanism,
stakeholder, threat, asset, and requirement), cost concepts (type, magnitude,
timing, uncertainty, and payer/beneficiary roles), and evidence concepts
(source type, confidence level, and provenance).
To reduce taxonomic ambiguity, class hierarchies are reviewed using
OntoClean-style meta-properties (rigidity, identity, dependence) where
applicable \cite{guarino2009ontoclean}.

For operational query stability, the current implementation uses a
\textit{category-as-individual} pattern for \textit{CostType}: each
\textit{CostInstance} links via \textit{hasCostType} to a controlled-vocabulary
individual (for example, \textit{MicroarchitecturalPerformanceCost}), rather
than relying on class/instance punning. This keeps CQ query behavior explicit in
OWL 2 DL tools and aligns directly with the tabular artifact schema.

\subsection{Representative Axioms}
To make formal commitments explicit, the current model uses the following
representative constraints. \textit{ProactivePosture},
\textit{ReactivePosture}, and \textit{HybridPosture} are pairwise disjoint
subclasses of \textit{SecurityPosture}. Every \textit{CostInstance} must have
at least one bearer and one realization time, and
\textit{hasActivationProfile} is restricted to \textit{AlwaysOn},
\textit{Conditional}, or \textit{EventTriggered}. Transfer semantics require
that an \textit{externalized} cost be borne in part by a stakeholder distinct
from the decision-maker. Opportunity-cost semantics require explicit
foregone-alternative descriptors (alternative-use identifier, foregone
resource/benefit, and design constraint).
A property chain (\textit{incursCost} followed by \textit{borneBy}) implies a
mechanism-level \textit{hasCostBearer} relation.

In description-logic style, a minimal fragment is:
\begin{equation}
\begin{aligned}
\textit{CostInstance} &\sqsubseteq \exists \textit{borneBy}.\textit{Stakeholder} \\
\textit{CostInstance} &\sqsubseteq \exists \textit{realizedAt}.\textit{TimeHorizon} \\
\textit{SecurityMechanism} &\sqsubseteq \exists \textit{hasSecurityPosture}.\textit{SecurityPosture}.
\end{aligned}
\end{equation}
In plain language, these lines say that every cost entry must identify who bears
it and when it appears, and every mechanism must declare a security posture.
These axioms are intentionally lightweight and can be tightened after
additional data mapping.
Under these constraints, reasoners can derive mechanism-level bearer facts from
cost instances without additional manual assertions.

\subsection{Validation Strategy}
We evaluate the ontology against three criteria: logical validity (consistency
and satisfiability under a DL reasoner), question coverage (whether CQs are
answerable by executable queries), and analytical utility (whether outputs
support comparative assessment across stakeholder-time matrices).
For empirical grounding, we map case-study data into normalized tuples
(\textit{mechanism}, \textit{cost type}, \textit{stakeholder}, \textit{time},
\textit{evidence}, \textit{source provenance}) and perform sensitivity analysis when probabilities or loss
magnitudes are uncertain.
Incident tuples are modeled separately to represent observed (or seed-modeled)
losses linked to weak or missing control families with confidence annotation.

\subsection{Success Criteria}
Beyond ontology completeness, evaluation requires three properties: decision
delta (allocation-aware analysis changes at least some conclusions relative to
local-overhead-only analysis), assumption visibility (weighting choices and
transfer penalties are explicit and contestable), and disconfirmability (the
paradigm claim specifies rejection conditions). Collectively, these criteria
evaluate whether burden-allocation instrument reasoning changes real analytical
outcomes.

\subsection{Measurement Protocol}
Each mapped cost instance includes a unit, uncertainty annotation, evidence
grade, source key, and source locator to improve reproducibility. Metric families include physical/resource
measures (percent area, watts, performance delta, memory overhead),
labor/process measures (engineer-months, verification campaign size, patch
turnaround), governance measures (audit effort, certification cycle length,
compliance frequency), and loss measures (incident frequency, recovery cost,
liability outlay, churn proxies).

Evidence quality is tracked using a three-level rubric: E1 for measured values,
E2 for model-based estimates, and E3 for elicited expert assumptions.
To avoid conflating confidence and provenance maturity, each row also carries a
\textit{data-origin} label (\textit{Measured}, \textit{Inferred}, or
\textit{Synthetic}) that is reported independently of evidence grade.
All reported comparisons state the proportion of E1/E2/E3 values and include a
sensitivity range when E2 or E3 dominates.

\subsection{Executable CQ Templates}
To demonstrate question coverage, we provide executable query templates for CQ1
through CQ8 in the companion artifact package and an artifact-generation script
that re-computes CQ outcomes from the seed datasets on every run
(\texttt{scripts/generate\_cq\_results.py}).
This script emits \texttt{artifacts/cq\_results.csv},
\texttt{artifacts/voi\_priorities.csv},
\texttt{artifacts/objective\_comparisons.csv},
\texttt{artifacts/shacl\_results.csv}, and
\texttt{artifacts/sensitivity\_rankings.csv}, which are then validated via
\texttt{make repro}. In the main paper, we report coverage and quality summaries
rather than inlining long query listings in two-column text.
